{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Our team's goal was to reconstruct the numerically valued part of the data that was missing to the best of our abilities. To this extent, we may claim that, although not as initially expected, we have succeeded in doing so. As it is made clear in our introduction, we studied 2 models as a means of solving this project: K-means clustering and DBScan, interpreted as a basic model and a non-trivial model, respectively. As the description states, our initial expectation was of the latter to outperform the former in providing plausible values for the reconstruction of missingness. However, our results definitively point towards the other way around being the case. Nevertheless, we consider DBScan's results a learning experience, and K-means a success in achieving our project's overarching goal.\n",
    "\n",
    "**Why did DBScan perform poorly?**\n",
    "\n",
    "The answer to this question lies in the method's algorithm. Without going into lengthy details that have already been covered both in our individual reflections, as well as in the references provided by every individual member of the group, the two models operate on quite different mechanisms. Firstly, K-means arbitrarily (or manually, if we would've chosen to take this approach) picks itself a center for operating. To the best of our understanding, and approximately 100 replicated runs of clusterings on the same data subsets where K-means _never_ picked a single NA dominated cluster -- the algorithm for the basic model assures that its center won't have missing values. This good property seems to guarantee that no cluster will only contain NAs, making the enrichment procedure impossible for that cluster. This, however, is not the case for DBScan, as we unfortunately unveil in Matt's work. His 5th row from our procedure was dominated by NAs: meaning that DBScan assigned to that cluster only missingness in our 3 target features. Once again, this is a consequence of the model's mathematical background. At a very simple explanatory level, DBScan is a less constricting method, since it uses a closeness based approach and absorbs points into clusters, making them bigger until convergence. Since so much of our data was missing to begin with, DBScan must've identified some properties in certain observations that classified them to a cluster, which happened if (and perhaps only if?) the data was missing in our target features - and created its final version for a cluster out of those. This, _we must stress_ is a remarkable result that's worth studying by itself, and DBScan does a fantastic job detecting this behaviour. However, for the purpose of our inquiry, this is as great of a failure as we could've gotten, since it renders the entire cluster pointless and unable to provide prediction on the missingness. \n",
    "\n",
    "This is only one instance where DBSCan failed to meet the properties we desired when asking for an enrichment process. Another good example would be the very similar way it handled outliers: Either really large or dead zero values. Part of that behaviour was to be expected, due to the nature of our dataset, but more versatility in its combinations would've avoided this occurrence. This retrospectively led us to believe that HDBScan was advisable in this situation, as it allowed for multiple densities of clusters. Regardless: Simplistic in its process, K-means delivered precisely due to the way it is constructed as a model, and for the same reason DBScan did not.\n",
    "\n",
    "**Where does this leave us?**\n",
    "\n",
    "This only leaves us to a sensible observation: Tools are only useful to the extent that they're used in the correct circumstance. One may argue it was a poor decision on the behalf of our team to begin with that we even chose DBScan as a non-trivial model knowing what our inquiry was about. However, we resign with the fact that our project's goal was nonetheless met, and we learned a valuable lesson in how to properly make use of classification models. Were we trying something else, such as outliers segregation or truthing of attacks or classifying missing data by type of missingness rather than reconstruction - it seems like DBScan would've immediately outperformed K-means. Both models still remain amazing tools for classification and neither should in any circumstance be discarded based on its failure to outperform the other in a given circumstance, since it's likely that the specific context made it impossible for it to shine. \n",
    "\n",
    "\n",
    "With these parting thoughts, we consider the project closed and the reconstruction as performed to a plausible result. Lastly, we'd like to thank all the contributors, whose profile links and their work on this project can be found below.\n",
    "\n",
    "\n",
    "## Contributors\n",
    "\n",
    "_Link to contributor's profile_   ~ _Link to their work on this project_\n",
    "\n",
    "\n",
    "\n",
    "[Alex Caian](https://github.com/Alex-Caian) ~ [Work #1](https://github.com/mc17336/DST-Assessment-2/tree/main/Alex%20Caian)\n",
    "\n",
    "[Matthew Corrie](https://github.com/mc17336) ~ [Work #2](https://github.com/mc17336/DST-Assessment-2/tree/main/Matt%20Corrie)\n",
    "\n",
    "[Wenqi Fang](https://github.com/fangwenqi9) ~ [Work #3](https://github.com/mc17336/DST-Assessment-2/tree/main/Wenqi%20Fang)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
